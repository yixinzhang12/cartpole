{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try1: no bug, RAM consumption too high, did not run successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "\n",
    "import gym # pull the environment\n",
    "\n",
    "import time # to get the time\n",
    "\n",
    "import math # needed for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 60000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    print(state)\n",
    "    print(np_array_win_size)\n",
    "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04085128  0.00973106 -0.04595046  0.0271315 ]\n",
      "[0.25 0.25 0.01 0.1 ]\n",
      "Episode: 0\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time() #set the initial time\n",
    "discrete_state = get_discrete_state(env.reset()[0]) #get the discrete start for the restarted environment \n",
    "done = False\n",
    "episode_reward = 0 #reward starts as 0 for each episode\n",
    "\n",
    "if episode % 2000 == 0: \n",
    "    print(\"Episode: \" + str(episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.random.random() > epsilon:\n",
    "\n",
    "    action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "else:\n",
    "\n",
    "    action = np.random.randint(0, env.action_space.n) #do a random ation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03654704,  0.4012201 , -0.05100159, -0.58633953], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02852264  0.5970179  -0.06272838 -0.8946423 ]\n",
      "[0.25 0.25 0.01 0.1 ]\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, done, _, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "episode_reward += reward #add the reward\n",
    "\n",
    "new_discrete_state = get_discrete_state(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "while not done: \n",
    "\n",
    "    if np.random.random() > epsilon:\n",
    "\n",
    "        action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "    else:\n",
    "\n",
    "        action = np.random.randint(0, env.action_space.n) #do a random ation\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "    episode_reward += reward #add the reward\n",
    "\n",
    "    new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "    if episode % 2000 == 0: #render\n",
    "        env.render()\n",
    "\n",
    "    if not done: #update q-table\n",
    "        max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "        current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "    discrete_state = new_discrete_state\n",
    "\n",
    "if epsilon > 0.05: #epsilon modification\n",
    "    if episode_reward > prior_reward and episode > 10000:\n",
    "        epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "        if episode % 500 == 0:\n",
    "            print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "t1 = time.time() #episode has finished\n",
    "episode_total = t1 - t0 #episode total time\n",
    "total = total + episode_total\n",
    "\n",
    "total_reward += episode_reward #episode total reward\n",
    "prior_reward = episode_reward\n",
    "\n",
    "if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
    "    mean = total / 1000\n",
    "    print(\"Time Average: \" + str(mean))\n",
    "    total = 0\n",
    "\n",
    "    mean_reward = total_reward / 1000\n",
    "    print(\"Mean Reward: \" + str(mean_reward))\n",
    "    total_reward = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00423244  0.02434653 -0.04185093  0.02503614]\n",
      "[0.25 0.25 0.01 0.1 ]\n",
      "Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yixinzhang/Desktop/cartpole/cptest/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;66;03m#do a random ation\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m#step action to get new states, reward, and the \"done\" status.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \u001b[38;5;66;03m#add the reward\u001b[39;00m\n\u001b[1;32m     23\u001b[0m new_discrete_state \u001b[38;5;241m=\u001b[39m get_discrete_state(new_state)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(EPISODES + 1): #go through the episodes\n",
    "    t0 = time.time() #set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()[0]) #get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 #reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) #do a random ation\n",
    "\n",
    "        new_state, reward, done, _, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "        episode_reward += reward #add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0: #render\n",
    "            env.render()\n",
    "\n",
    "        if not done: #update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: #epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() #episode has finished\n",
    "    episode_total = t1 - t0 #episode total time\n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward #episode total reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04627293,  0.00452833, -0.04643157, -0.01921855], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cptest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
